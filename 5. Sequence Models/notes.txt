SEQUENCE MODELS



Recurrent Neural Networks

[Why Sequence Models?]
Exemplos:
	- Speech Recognition: Transcrição de áudio
	- Music Generation: Geração de música dadas algumas notas iniciais
	- Sentiment Classification: Dada uma resenha, estimar a quantidade de estrelas
	- DNA Sequence Analysis: Identificar proteínas
	- Machine Translation: Tradução de texto de um idioma para outro
	- Video Activity Recognition: Identificar que tipo de atividade um humano está realizando num vídeo (ex: correndo)
	- Name Entity Recognition: Dado um texto, reconhece as entidades envolvidas (ex: pessoas, empresas, ...)

[Notation]
Exemplo com Name Entity Recognition:
	- x: Harry Potter and Hermione Granger invented a new spell.
		-> x = [x<1>, x<2>, ..., x<t>, ..., x<9>]
			- t de temporal
			- Tx é o tamanho da sequência (Tx = 9)
	- y: [1, 1, 0, 1, 1, 0, 0, 0, 0]
		-> Ty = 9
Palavras são representadas com um vocabulário:
	[a, aaron, ..., and, ..., harry, ..., potter, ..., zulu]
	(1    2         367       4075        6830         10000) <- índices

	Então as entradas de x são da forma (one-hot vectors):
	x<1> = [0, 0, ..., 0, 1, 0, ..., 0]    (Harry)
		   (1  2          4075       10000) <- índices

[Recurrent Neural Network Model]
Não usamos uma NN comum pois: 
	- as entradas e saídas podem ter diferentes tamanhos de acordo com cada amostra
	- as features aprendidas precisam ser compartilhadas através de diferentes posições no texto

Redes neurais recorrentes (Tx = Ty  [arquitetura many-to-many]):
		yhat<1>		yhat<2>					yhat<Ty>
		  | Wya		  | Wya					  | Wya
		| . |		| . |					| . |
  a<0>	| . | a<1>	| . | a<2>		a<Tx-1>	| . |
------->| . |------>| . |------> ... ------>| . |
		| . |  Waa	| . |  Waa		 Waa	| . |
		| . |		| . |					| . |
		  | Wax		  | Wax					  | Wax
		 x<1>		 x<2>					x<Tx>

	OBS: a<0> normalmente é um vetor de zeros
	OBS2: W's são parâmetros

	-> Desvantagem: Utiliza apenas as palavras anteriores para classificar

Forward Prop:
	a<0> = vetor de zeros
	a<1> = g1(Waa * a<0> + Wax * x<1> + ba)		<- tanh/ReLU
	yhat<1> = g2(Wya * a<1> + by)				<- sigmoid/softmax
	...
	a<t> = g1(Waa * a<t-1> + Wax * x<t> + ba) = g1(Wa[a<t-1>, x<t>] + ba)   [simplificação (Wa = [Waa | Wax])]
	yhat<t> = g2(Wya * a<t> + by) = g2(Wy * a<y> + by)

[Backpropagation Through Time]
Loss:
	L<t>(yhat<t>, y<t>) = - y<t> * log(yhat<t>) - (1-y<t>) * log(1-yhat<t>)
	L(yhat, y) = sum(L<t>(yhat<t>, y<t>) para t de 1 a Ty)

[Different Types of RNNs]
-> Para classificação de sentimento temos Ty = 1 < Tx  [arquitetura many-to-one]
	- Nesse caso temos yhat saindo apenas da última camada da rede

-> Para geração de música temos Tx = 1 < Ty  [arquitetura one-to-many]
	- Nesse caso temos x entrando apenas na primeira camada da rede 
	- Conectamos a saída yhat<t-1> como se fosse a entrada x<t> da camada t
	- OBS: x pode ser vazio

-> Para tradução de máquinas temos Tx != Ty  [arquitetura many-to-many]
	- Nesse caso temos primeiro Tx camadas apenas com entradas (sem saídas yhat) [parte encoder] e Ty camadas só com saídas (sem entradas x) [parte decoder]

[Language Model and Sequence Generation]
Para speech recognition queremos uma rede que forneça boas probabilidades:
	-> Por exemplo, "The apple and pear salad" pode ser interpretado como:
		- The apple and pair salad: P(The apple and pair salad) = 3.2 * 10^-13
		- The apple and pear salad: P(The apple and pear salad) = 5.7 * 10^-10  <- mais provável

	-> Language modelling fornece: 
		P(sentença) = P(y<1>, y<2>, ..., y<Ty>)

Conjunto de treinamento: Um grande corpus de texto
	- Substituímos paralavras desconhecidas pelo token <UNK>
	- Podemos incluir pontuação:
		- Ex: ponto final '.' pode ser substituído por <EOS>

Modelo RNN:
"Cats average 15 hours of sleep a day. <EOS>"

[P(a), P(aaron), ...]   [P(_|"cats")]   [P(_|"cats average")]
		yhat<1>       yhat<2>     	yhat<3>			<-(softmax)
		  |     		|				|
a<0> --> a<1> -------> a<2>	--------> a<3> ---> ...
		  |		        |       		|
	  x<1> = 0       x<2> = y<1>  x<3> = y<2>

L<t>(yhat<t>, y<t>) = - sum(y_i<t> * log(yhat_i<t>))
L(yhat, y) = sum(L<t>(yhat<t>, y<t>))

OBS: Dadas as primeiras palavras, a rede consegue dizer qual a probabilidade de cada palavra do vocabulário ser a próxima no texto

[Sampling Novel Sequences]
Para gerar novas sequências basta, para cada camada t: 
	- sortearmos a saída yhat<t> (dentre as palavras com alta probabilidade) 
	- considerarmos y<t> = yhat<t> para a próxima camada (x<t+1> = yhat<t>)
	OBS1: Paramos quando gerarmos o token <EOS>
	OBS2: Podemos rejeitar <UNK> caso seja sorteada

Modelo de liguagem a nível de caracter:
	- Nesse caso utilizamos o vocabulário: ['a', 'b', ..., 'z', ' ', '.', ',', ';', '0', ..., '9', 'A', ..., 'Z']
	- Vantagem: Não existem palavras desconhecidas
	- Desvantagem: Sequências mais longas (mais computacionalmente custoso de treinar)

[Vanishing Gradients with RNNs]
O modelo que geramos até o momento não é muito bom para captar a dependência de palavras muito distantes
Ex:
	The CAT, which ate ..., WAS full
	The CATS, which ate ..., WERE full

Isso ocorre por causa do desaparecimento de gradientes quando a Rede Neural é muito grande

Explosão de gradientes é mais fácil de identificar pois veremos diversos NaNs nos gradientes
	-> Solução: Gradient Clippling
		- Reescalar os grandientes quando forem maiores que algum threshold

[Gated Recurrent Unit (GRU)] 	[Cho et al., 2014. On properties of neural machine translation: Encoder-decoder approaches]		[Chung et al., 2014. Empirical Evaluation of Gated Recurrent Neurak Networks on Sequence Modeling]
Para resolver o desaparecimento de gradientes podemos utilizar uma célula de memória (c) que é atualizada a cada camada:
	-> GRU simplificada:
		~c<t> = tanh(Wc[c<t-1>, x<t>] + bc)		[novo candidato a c]
		ru = sigmoid(Wu[c<t-1>, x<t>] + bu)		[taxa de atualização (gate)]
		c<t> = ru * ~c<t> + (1-ru) * c<t-1>		[aqui * é element-wise]
		a<t> = c<t>

	-> GRU completa:
		~c<t> = tanh(Wc[rr * c<t-1>, x<t>] + bc)		[novo candidato a c]
		ru = sigmoid(Wu[c<t-1>, x<t>] + bu)				[taxa (gate) de atualização]
		rr = sigmoid(Wr[c<t-1>, x<t>] + br)				[taxa de quanto c<t-1> influencia em ~c<t>]
		c<t> = ru * ~c<t> + (1-ru) * c<t-1>				[aqui * é element-wise]
		a<t> = c<t>

[Long Short Term Memory (LSTM)]		[Hochreiter & Schmidhuber 1997. Long short-term memory]
Uma versão mais genérica da GRU:
	~c<t> = tanh(Wc[c<t-1>, x<t>] + bc)			[novo candidato a c]
	ru = sigmoid(Wu[c<t-1>, x<t>] + bu)			[taxa de atualização (update)]
	rf = sigmoid(Wf[c<t-1>, x<t>] + bf)			[taxa de esquecimento (forget)]
	ro = sigmoid(Wo[c<t-1>, x<t>] + bo)			[taxa de saída (output)]
	c<t> = ru * ~c<t> + (1-ru) * c<t-1>			[aqui * é element-wise]
	a<t> = ro * tanh(c<t>)

[Bidirectional RNNs (BRNNs)]
A inspiração é "obter informação do futuro"
Basicamente temos 2 redes:
    - forward: equivalente ao que temos construído nas últimas aulas
    - backward: avaliada no sentido contrário

Calculamos yhat compondo as duas ativações:
    yhat<t> = g(Wy[a_f<t>, a_b<t>] + by)        
        -> a_f: ativação forward
        -> a_b: ativação backward

-> Desvantagem: Precisa da sequência inteira para fazer predições

[Deep RNNs]
A ideia é empilhar diversas camadas (linhas) de RNNs e conectar a saída da camada anterior na entrada da camada atual
Ex:
    a2<3> = g(Wa2[a2<2>, a1<3>] + ba2)



Natural Language Processing & Word Embeddings

-> Introduction to Word Embeddings

[Word Representation]
Até agora temos usado a representação em vetores "one-hot"
O problema é que o modelo não aprende a associar palavras como por exemplo "laranja" e "maçã" (reconhecer que são frutas)
Uma forma de trazer essa semântica ao modelo é criar uma representação com várias features para cada palavra (por exemplo: gênero [-1 para masculino e 1 para feminino], comida (0, 1), tamanho (0, 1), custo (0, 1), verbo (0, 1), ...)
Após a construção dessa matriz (word embedding) podemos visualizar os dados utilizando o algoritmo t-SNE [van der Maaten and Hinton., 2008. Visualizing data using t-SNE]

[Using Word Embeddings]
É possível utilizarmos transferência de aprendizado com base em um grande dataset não classificado para treinarmos nossa rede:
	- Aprender "word embeddings" de um grande corpus textual (1 - 100 bilhões de palavras) [ou baixar "word embeddings" pré-treinados da internet]
	- Transferir o "embedding" para uma nova tarefa, com um conjunto de treinamento menor (100K palavras)
	- Continuar ajustando os "embeddings" com dados novos (opcional)

"Word embeddings" se relacionam com "encoding" (citado nas aulas de reconhecimento facial)

[Properties of Word Embeddings]
Utilizando word embeddings podemos identificar analogias, como por exemplo:		[Mikolov et. al., 2013, Linguistic regularities in continuos space word representations]
"Homem" está para "Mulher" assim como "Rei" está para _____ (Man->Woman as King->?):
	e_5391 = e_man = [-1, 0.01, 0.03, 0.09]   (vetor do embedding da palavra "man", que está no índice 5391)
	e_woman = [1, 0.02, 0.02, 0.01]
	e_king = [-0.95, 0.93, 0.70, 0.02]
	Queremos que: e_man - e_woman ~ e_king - e_?
	Isso é, queremos encontrar a palavra w que satisfaz: argmax_w(sim(e_w, e_king - e_man + e_woman))  
		e_man - e_woman ~ [-2, 0, 0, 0]
		e_king - e_queen ~ [-2, 0, 0, 0]
	(acurácia de 30% - 75% utilizando esse método)

A similaridade (sim) normalmente usada é a similaridade de cossenos:
	sim(u, v) = u.T * v / (||u|| * ||v||)

[Embedding Matrix]
Exemplo: Matriz E de embeddings de 300 x 10000	[300 features de 10000 palavras]
	Suponhamos que a palavra "orange" esteja na coluna 6257, então:
		E * o_6257 = e_6257		[dimensão 300 x 1]			(o_6257 é o vetor one-hot com 1 na posição 6257 e dimensão 10000 x 1)
		(na prática é melhor usar uma função especializada para buscar pela coluna contendo o embedding, pois a matriz e o vetor one-hot normalmente possuem dimensões grandes)

-> Learning Word Embeddings: Word2Vec & GloVe

[Learning Word Embeddings]
Historicamente os primeiros algoritmos usados para o aprendizado de word embeddings eram mais complexos e, com o tempo, descobriu-se que algoritmos mais simples também poderiam funcionar muito bem
Rede neural comum:		[Bengio et. al., 2003, A neural probabilistic language model]
	- Usa as primeiras k palavras para prever a próxima (k é hiperparâmetro)
	Exemplo: "I want a glass of orange _____"
		(Índices das palavras no vocabulário: 4343, 9665, 1, 3852, 6163, 6257)
		-> Pegamos os k=6 embeddings das palavras (e_4343, ..., e_6257) 
		-> Juntamos os embeddings em um vetor de 1800 x 1 (cada palavra tem 300 features)
		-> Usamos esse vetor como entrada de uma rede neural com uma hidden layer com parâmetros W1 e b1
		-> A rede tem como saída uma camada softmax de tamanho 10000 (tamanho do nosso vocabulário)

[Word2Vec]
Skip-grams:		[Mikolov et. al., 2013, Linguistic regularities in continuos space word representations]
	Exemplo: "I want a glass of orange juice to go along with my cereal"
	- Ideia: sortear uma palavra (c) da frase para usar como contexto e algumas palavras alvo (t) antes ou depois dela (dentro de uma janela determinada)
		Context 	Target
		orange		juice
		orange		glass
		orange		my
	- A partir da palavra c obtemos e_c e utilizamos e_c como entrada de uma camada softmax
	- Softmax: p(t | c) = e^(theta_t.T * e_c) / sum(e^(theta_j.T * e_c) para j de 1 até 10000)		[theta_t é o parâmetro associado com a saída t]
	- Loss: L(y, yhat) = - sum(y_i * log(yhat_i) para i de 1 até 10000)
	- Problema: Podemos ter problemas de desempenho computacional por conta de a camada softmax ser muito grande
		-> Usar hierarchical softmax (uma árvore contendo as palavras mais frequentes nos nós mais rasos) [mais detalhes no artigo referenciado]
	- Importante: A palavra c deve ser sorteada evitando palavras muito frequentes (tais como the, of, a, and, to, ...)
		-> Ou seja, não sorteamos da distribuição uniforme pois analisaríamos as mesmas palavras (as mais frequentes) muitas vezes

OBS: a definição usada para "estar no contexto" é +-k palavras (isso é, k palavras anteriores ou posteriores à palavra context)

[Negative Sampling]		[Mikolov et. al., 2013, Linguistic regularities in continuos space word representations]
Definimos um novo problema de aprendizado, obtendo um exemplo onde uma palavra é target de outra e sorteando do vocabulário k exemplos "não-target"
Exemplo: "I want a glass of orange juice to go along with my cereal" (k = 4)
	Context 	Word 	Target?
	orange		juice	1
	orange		king 	0
	orange		book 	0
	orange		the 	0
	orange		of	 	0

	------- x -------	y 		(Context (c) e Word (t) compõem a entrada x do aprendizado e y a saída)
	c 			t 		y

Escolhendo k:
	- Datasets pequenos/médios: 5-20
	- Datasets grandes: 2-5

Para não usarmos softmax (resultando no mesmo problema acima), treinaremos 10000 regressões logísticas: P(y=1|c,t) = sigmoid(theta_t.T * e_c)
	- A cada iteração k + 1 unidades (palavras) são treinadas

OBS: No sorteio das palavras do vocabulário para as quais "Target?" é zero usamos a heurística:
	P(wi) = f(wi)^(3/4) / sum(f(wj)^(3/4) para j de 1 até 10000)

[GloVe Word]		[Pennington et. al., 2014. Glove: Global vectors for word representation]
X_i_j = número de vezes que a palavra j aparece no contexto da palavra i 	(i e j são equivalentes a c e t da outra formulação)
Custo: sum(sum(f(X_i_j) * (theta_i.T * e_j + bi + bj' - log(X_i_j))^2))
	f(X_i_j) = 0 se X_i_j = 0
	caso contrário, dá mais peso a palavras mais frequentes
e_w final = (e_w + theta_w) / 2

OBS: Na matriz gerada por esse algoritmo pode ser difícil visualizar o papel de cada feature pois os vetores gerados podem estar em outra base

-> Applications Using Word Embeddings

[Sentiment Classification]
Dado um texto (x) determinar se o autor gostou ou não (y) daquilo sobre o que ele está falando

Classificação de sentimento simples:
	- Mesmo com um dataset pequeno (10000-100000 palavras), podemos usar um word embedding de 100B de palavras para realizar a tarefa
	Exemplo: resenhas de restaurantes
	"The dessert is excellent" (8928, 2468, 4694, 3180)
	- Obtemos os embeddings (e_8928, ..., e_3180) de dimensão 300 x 1
	- Calculamos a média (ou soma) dos componentes entre todos os embeddings (dimensão do resultado também é 300 x 1)
	- Passamos o resultado para uma camada softmax para 5 classes (de 1 a 5 estrelas) para determinar yhat
	- Desvantagem: esse algoritmo ignora a ordem das palavras
		-> Exemplo problemático: "Completely lacking in good taste, good service and good ambience" 
			- Será classificado com muitas estrelas por conta da quantidade de vezes que "good" foi usada
			- Normalmente "good" é associada a um sentimento positivo

RNN para classificação de sentimento:
	- Os e_j são usados como entrada no lugar dos vetores "one-hot" o_j
	- Na última "camada" temos a saída sendo passada para uma camada softmax [arquitetura many-to-one]

[Debiasing Word Embeddings]		[Bolukbasi et. al., 2016. Man is to computer as woman is to homemaker? Debiasing word embeddings]
Aqui "bias" não é o viés do qual normalmente falamos para discutir técnicas de ML (bias/variance), mas sim de um bias dados por gênero, etnia, idade, orientação sexual, ...
Exemplo:
	Man:Woman as King:Queen 					(correto)
	Man:Computer_Programmer as Woman:Homemaker 	(incorreto)
		- Estereótipo que pode ter sido dado pelos textos presentes no conjunto de treinamento usado no treino do word embedding
		- Queremos: Man:Computer_Programmer as Woman:Computer_Programmer

Para reduzirmos ou eliminarmos um determinado bias identificado:
	- Identificamos a direção (vetorial) do bias
		-> Tirar uma média de: e_he - e_she, e_male - e_female, ...
	- Neutralizar: Para cada palavra que não possui intrinsicamente um gênero (doctor, babysitter, ...) obtemos sua projeção de forma a nos livrarmos do bias 
	- Equalizar pares: 
		-> Obter pares de palavras como (mother, father), (girl, boy), ...
		-> Equalizar para que cada um desses pares só difiram entre si na direção do bias e sejam iguais nas outras

	OBS: Um classificador linear pode nos dizer quais palavras passar pela etapa de neutralização



Sequence Models & Attention Mechanism

-> Various Sequence to Sequence Architectures

[Basic Models]
Modelo de sequência para sequência (traduções):
	- Artigos: 
		-> Sutskever et al., 2014. Sequence to sequence learning with neural networks
		-> Cho et al., 2014, Learning phrase representations using RNN encoder-decoder for statistical machine translation
	- Objetivo:
		-> x = "Jane visite l'Afrique en septembre" => y = "Jane is visiting Africa in September"
	- Arquitetura:
		-> Rede encoder (uma unidade por entrada em x) -> Rede decoder (uma unidade por saída em y)

Geração automática de descrições de imagens:
	- Artigos:
		-> Mao et. al., 2014. Deep captioning with multimodal recurrent neural networks
		-> Vinyals et. al., 2014. Show and tell: Neural image caption generator
		-> Karpathy and Li, 2015. Deep visual-semantic alignments for generating image descriptions
	- Objetivo:
		-> x = imagem de um gato sentado em uma cadeira => y = "A car sitting on a chair"
	- Arquitetura:
		-> Rede encoder -> Rede decoder
			- Encoding da imagem pode ser obtido passando-a por uma rede convolucional treinada (tal como AlexNet) e removendo a camada softmax [geração de feature de tamanho 4096]
			- Parte decoder é uma RNN que tem como entrada o encoding gerado (uma unidade por saída de y)

[Picking the Most Likely Sentence]
A tradução de máquina pode ser enxergada como um language model (como os que construímos na primeira semana) condicionado a entrada x (data pela parte encoder da frase em francês)

Encontrar a tradução mais provável:
	- Exemplo:
		-> x = "Jane visite l'Afrique en septembre"
		-> y pode ser: 
			- "Jane is visiting Africa in September" [1]
			- "Jane is going to be visiting Africa in September" [2]
			- "In September, Jane will visit Africa" [3]
			- "Her African friend welcomed Jane in September" [4]

	- Queremos argmax [y<1>, ..., y<Ty>] P(y<1>,...,y<Ty> | x)	[beam search é usado]
		-> Não usamos greedy search pois ela não funciona tão bem (poderia gerar a saída [2], por exemplo)
			- P(Jane is visiting | x) < P(Jane is going | x) pois going é uma palavra mais comum que visiting

[Beam Search]
Parâmetro B = 3 (beam width)
	- Isso significa que a cada passo instanciamos 3 cópias da rede para avaliarmos as senteças parciais referentes à busca
Primeiro passo: O algoritmo pega as B palavras com melhores probabilidades como possíveis primeiras palavras da saída (y<1>)
	- Exemplo: Selecionamos as palavras [in, Jane, September]
Segundo passo: 
	- Para cada um dos B y<1> possíveis, setar yhat<1> para essa palavra 
	- Calcular os B melhores yhat<2> usando: P(y<1>, y<2> | x) = P(y<1> | x) * P(y<2> | x, y<1>)
		OBS: Podemos acabar descartando algum y<1> anteriormente possível (isso é, pegamos as B melhores sentenças que maximizam P(y<1>, y<2> | x))
	- Exemplo: Selecionamos as sentenças ["in september", "Jane is", "Jane visits"]		(September foi descartado)
Terceiro passo: ...
(B = 1 => Greedy search)

[Refinements to Beam Search]
Melhores resultados pode ser obtidos com algumas alterações no algoritmo básico descrito na aula anterior
Normalização de tamanho:
	- O cálculo de probabilidades é dado por: argmax(y) Prod(P(y<t> | x, y<1>,...,y<t-1>) para t de 1 até Ty)
	- Esse valor pode ser muito pequeno conforme Ty aumenta por conta da multiplicação de diversos valores menores que 1 (pode resultar em underflow)
	- Para evitar esse problema podemos somar os logs (equivalente para selecionar o máximo):  argmax(y) sum(log(P(y<t> | x, y<1>,...,y<t-1>)) para t de 1 até Ty)
		-> Tende a escolher frases pequenas, para melhorar normalizamos multiplicando por 1/(Ty^alpha)  [alpha entre 0 e 1 (hiperparâmetro), normalmente 0.7]

Escolhendo B:
	- B grande => Melhores resultados, mas mais lento
	- B pequeno => Resultados piores, mas mais rápido
	- Em produção normalmente é usado B entre 10 e 100, dependendo da aplicação
	- Para pesquisa normalmente se usa B entre 1000 e 3000 para se ter uma boa performance

OBS: Diferentemente de outros algoritmos de busca como BFS e DFS, não é garantido que o máximo será encontrado com o uso de Beam Search (mas executa muito mais rápido)

[Error Analysis in Beam Search]
Exemplo:
	- x = "Jane visite l'Afrique en septembre"
	- Tradução humana: "Jane visits Africa in September" (y*)
	- Algoritmo: "Jane visited Africa last September" (yhat)
	- Análise de erro: Computamos P(y*|x) e P(yhat|x) e os comparamos
		-> Caso 1: P(y*|x) >= P(yhat|x)
			- Conclusão: É culpa do Beam Search pois ele escolhe yhat mesmo P(y*|x) sendo maior
		-> Caso 2: P(y*|x) < P(yhat|x)
			- Conclusão: A culpa é do modelo RNN pois y* é uma tradução melhor e obteve probabilidade menor do que yhat
	- Realizando a análise de erro para todos os exemplos de erro do conjunto dev podemos montar uma tabela e descobrir a proporção de erros dados pelo Beam Search vs erros dados pela RNN
	OBS: esse método pode ser usado para qualquer tipo de busca heurística implementada

[Bleu Score]		[Papineni et. al., 2002. A method for automatic evaluation of machine translation]
Para tradução de francês para inglês podemos ter diversas respostas igualmente boas
Para determinar qual escolher, usamos o Bleu score (Blue - Bilingual Evaluation Understudy)
Exemplo:
	x = "Le chat est sur le tapis"
	Referências (qualquer uma delas é boa o suficiente):
		- Referência 1: "The cat is on the mat"
		- Referência 2: "There is a cat on the mat"
	Saída: the the the the the the the
		-> Precisão: Quantas palavras da saída aparecem nas referências
			- Nesse caso: 7/7 = 1
		-> Precisão modificada: Limita para o máximo de vezes (count_clip) que a palavra apareceu entre as frases das referências
			- Nesse caso: 2/7

Bleu score em bigramas:
	- Referência 1: "The cat is on the mat"
	- Referência 2: "There is a cat on the mat"
	- Saída: The cat the cat on the mat
		Bigrama 	count 	count_clip
		the cat 	2		1
		cat the 	1		0
		cat on 		1		1
		on the 		1		1
		the mat 	1		1
		-> Precisão: 4/6

Bleu score para unigramas:
	P1 = sum(count_clip(unigrama) para cada unigrama em yhat) / sum(count(unigrama) para cada unigrama em yhat)

Bleu score para n-gramas:
	Pn = sum(count_clip(n-grama) para cada n-grama em yhat) / sum(count(n-grama) para cada n-grama em yhat)

Podemos calcular P1, P2, P3 e P4 e considerar todos para gerar o score:
	Blue score combinado = BP * exp(sum(Pn para n de 1 até 4) / 4)
	BP = brevity penalty
		-> = 1 se o tamanho da saída for menor que da referência
		-> = exp(1 - reference_output_length / MT_output_length) caso contrário

[Attention Model Intuition]		[Bahdanau et. al., 2014, Neural machine translation by jointly learning to align and translate]
É difícil memorizar toda a sentença para entradas muito grandes e isso se reflete no desempenho do modelo conforme o número de palavras aumenta
Humanos normalmente realizam traduções de uma frase (ou um pedaço dela) por vez e Attention Model se inspira nesse comportamento
A ideia básica é que cada palavra da saída possui um contexto (uma quantidade de palavras da entrada das quais ela pode depender)
OBS: é usada uma rede bidirecional

[Attention Model]		[Bahdanau et. al., 2014, Neural machine translation by jointly learning to align and translate]
Usamos uma rede bidirecional (GRU ou LSTM) para extrair features do texto em francês a ser traduzido
Para simplificar a notação, definiremos: 
	t': usado para indexar as palavras do texto em francês
	a<t'> = (a_f<t'>, a_b<t'>)		[(a<t'> forward, a<t'> backward)]

Temos uma outra rede unidirecional que dará as saídas das palavras em inglês:
	- Cada unidade S<t> depende de S<t-1>, y<t-1> e uma entrada c<t>, e tem como saída yhat<t>
	- A entrada c<t> é uma soma das features de 3 palavras, dadas pela outra rede
		-> alpha<1,1>, alpha<1,2>, alpha<1,3>:		[attention weights: alpha<t,t'> é "a quantidade de atenção que y<t> deveria prestar em a<t'>"]
			- Restrição: sum(alpha<1, t'> para todo t') = 1
			- c<1> = sum(alpha<1,t'> * a<t'> para todo t')
	- Computando alpha<t,t'>:
		-> alpha<t,t'> = exp(e<t,t'>) / sum(exp(e<t,t'>) para t' de 1 até Tx)
		-> Uma rede neural pequena é treinada para calcular os vetores e<t,t'> tendo como entrada s<t-1> e a<t'>


-> Speech Recognition - Audio Data

[Speech Recognition]
Dado um áudio (x) queremos gerar uma transcrição (y)
Para resolver esse problema, temos duas opções:
	- Attention Model
	- Custo CTC (Connectionist Temporal Classification) para reconhecimento de fala 	[Graves et. al., 2006. CTC: Labeling unsegmented sequence data with RNN]
		-> Ideia básica: saída com o mesmo tamanho da entrada
		-> Gera uma saída no formato: ttttt_h_eeee___ ___qqqq...
			- Regra: Colapsar caracteres repetidos não separados por "blank"

[Trigger Word Detection]
Exemplos: "Okay Google", "Hey Siri", "Alexa"
A saída tem o mesmo tamanho da entrada, quando y<t> = 1 quer dizer que a palavra/frase gatilho foi falada
OBS: O conjunto de treino pode ficar muito desbalanceado (mais 0 do que 1)
	-> Um "hack" para facilitar o treinamento é setar diversos 1's após a palavra ter sido falada (no lugar de setar apenas um para um tempo t específico)
