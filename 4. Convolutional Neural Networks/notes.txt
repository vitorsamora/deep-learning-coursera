CONVOLUTIONAL NEURAL NETWORKS



Foundations of Convolutional Neural Networks

[Computer Vision]
Exemplos:
    - Classificação de imagens -> é a foto de um gato ou não?
    - Detecção de objetos -> posição de um ou mais carros em uma imagem
    - Neural Style Transfer -> "composição" entre duas imagens, uma contendo uma foto e outra contendo algum estilo para ser aplicado

Em imagens grandes (1000x1000 pixels) teremos milhões de unidades na camada de entrada, o que implica W1 com bilhões de parâmetros
    -> O treinamento fica computacionalmente inviável e se torna difícil previnir o overfitting por conta da grande quantidade de features

Para Visão Computacional, queremos poder treinar com imagens muito grandes e isso pode ser feito através do processo de convolução

[Edge Detection Example]
Ideia básica para detectar linhas verticais ou horizontais:
    - Criar um filtro (kernel) na forma [[1 0 -1] [1 0 -1] [1 0 -1]]   (detecção de linha vertical)
    - Fazer a convolução (*) entre o filtro (3x3) e a imagem (6x6) gerando uma matriz (4x4)
        -> Multiplicação elemento a elemento entre o filtro e cada submatriz 3x3 e soma entre todos os elementos

Convolução:
    - Python: conv_forward
    - Tensorflow: tf.nn.conv2d
    - Keras: Conv2D

[More Edge Detection]
 A matriz resultante possui valores muito grandes ou muito pequenos (negativos) nos locais em que a convolução foi feita com uma linha vertical (pode pegar o valor absoluto)
 Outros filtros:
    - Sobel filter: [[1 0 -1] [2 0 -2] [1 0 -1]]  (coloca mais peso nos pixels centrais)
    - Scharr filter: [[3 10 -3] [10 0 -10] [3 0 -3]]

Para o aprendizado, tornamos os valores do filtro parâmetros w1, ..., w9 para que o algoritmo aprenda o melhor filtro para a classificação

[Padding]
Modificação no algoritmo de convolução para facilitar o manuseio das matrizes resultantes
Dimensões:
    - imagem: nxn
    - filtro: fxf
    - convolução: n-f+1 x n-f+1

Desvantagens: 
    - toda vez que um filtro é aplicado a matriz diminui
    - alguns pixels são usados na computação de várias células e alguns são usados apenas uma vez

Para solucionar esses problemas basta incluir uma borda de p pixels com zeros na imagem original antes de realizar a convolução

Valid Convolution: sem padding  (n-f+1 x n-f+1)
Same Convolution: com padding para que a imagem de entrada tenha o mesmo tamanho da imagem de saída  
    (n+2p-f+1 x n+2p-f+1) = (n x n)  
    =>  p = (f - 1) / 2   [f normalmente é ímpar]

[Strided Convolutions]
Exemplo: Stride = 2
    No lugar de computar os produtos e somas a cada 1 pixel, pulamos de 2 em 2

Dimensões:
    - imagem: nxn
    - filtro: fxf
    - padding: p
    - stride: s
    -> resultado: floor(1 + (n+2p-f) / s)   x   floor(1 + (n+2p-f) / s)

OBS: Em livros de matemática a definição formal de convolução diz que antes de realizarmos a convolução e necessário espelhar (flip) a matriz do filtro no sentido vertical e depois espelhá-la no sentido horizontal
    -> Esse processo é feito para que a convolução seja associativa   [essa propriedade não importará para as aplicações de NN]
    -> Tecnicamente o que fizemos antes é chamado de correlação cruzada, não convolução (mas a convenção em Deep Learning é chamar de convolução)

[Convolutions Over Volume]
Convolução para RGB (3 dimensões):
    - imagem: 6x6x3 [altura, largura, # canais (profundidade)]
    - filtro: 3x3x3
    - convolução: 4x4x1
No lugar de computar um valor para cada submatriz de 3x3 para a convolução (como no exemplo anterior), multiplicamos elemento a elemento os "subcubos" da imagem com o cubo do filtro e somamos (os 3*3*3 = 27 valores) para computar uma entrada na matriz de convolução
OBS: Podemos fazer a convolução da imagem com diversos filtros e enfileirar (como se fossem canais) os resultados em uma única saída

Genericamente:
    - imagem: n x n x n_c [número de canais]
    - filtro: f x f x n_c
    - convolução: n - f + 1 x n - f + 1 x n_f [número de filtros]

[One Layer of a Convolutional Network]
Exemplo:
    - imagem: 6x6x3
    - filtros (2 filtros): 3x3x3
    - convolução 
        -> gera duas matrizes c1 e c2 de 4x4 
        -> fazemos o cálculo de a1_1 e a1_2  
            - a1_1 = ReLU(c1 + b1) (b1 e b2 vetores de bias, equivalente a: a_1 = sigmoid(W1 * a0 + b1))
            - a1_2 = ReLU(c2 + b2)
    - saída: a1 = [a_1, a_2] 4x4x2  (próxima camada da rede)

Resumo da notação para uma camada l de convolução:
    - fl: tamanho do filtro
    - pl: padding
    - sl: stride
    - entrada: n_h{l-1} x n_w{l-1} x n_c{l-1}
    - saída: n_hl x n_wl x n_cl
        -> n_hl = floor(1 + (n_h{l-1} + 2*pl - fl) / sl)
        -> n_wl = floor(1 + (n_w{l-1} + 2*pl - fl) / sl)
    - cada filtro é: fl x fl x n_c{l-1}
    - ativações: n_hl x n_wl x n_cl   =>  Al é m x n_hl x n_wl x n_cl
    - pesos (W): fl x fl x n_c{l-1} x n_cl
    - bias (b): n_cl  ->  (1,1,1,n_cl)

[Simple Convolutional Network Example]
Exemplo:
    - imagem: 39x39x3
        -> n_h0 = n_w0 = 39; n_c0 = 3
        -> f1 = 3, s1 = 1, p1 = 0, 10 filtros
    - saída da camada 1: 1 + (n + 2*p - f)/s = 37
        -> n_h1 = n_w1 = 37
        -> n_c1 = 10
    - camada 2:
        -> f2 = 5, s2 = 2, p2 = 0, 20 filtros
    - saída da camada 2:
        -> n_h2 = n_w2 = 17
        -> n_c2 = 20
     - camada 3:
        -> f3 = 5, s3 = 2, p3 = 0, 40 filtros
    - saída da camada 3:
        -> n_h3 = n_w3 = 7
        -> n_c3 = 40
    - unroll da saída da camada 3
    - camada softmax / regressão logística
    - saída yhat

Tipos de camadas em uma CNN:
    - convolution (CONV)
    - pooling (POOL)
    - fully conected (FC)

[Pooling Layers]
Usadas para reduzir o tamanho da representação, acelerar a computação e tornar as features de detecção mais robustas

Exemplo de pooling: Max Pooling
    - entrada: 4x4
    - saída: 2x2
    -> divide a matriz de entrada em 4 submatrizes e pega o máximo de cada região  (s = 2, f = 2)
    -> intuição: valores grandes indicam a ativação de alguma feature
    -> vantagem: dados f e s (hiperparâmetros) não necessita de outros parâmetros

Outro exemplo: Average Pooling
    -> a mesma coisa só que usando a média no lugar do máximo

OBS: por si só uma camada de pooling não é considerada uma camada da NN por não possuir pesos associados

[CNN Example]
(Exemplo inspirado na LeNet-5)
Entrada: imagem de 32x32x3 (RGB)
Objetivo: identificar números na imagem
Camada 1:
    CONV1:
        f = 5, s = 1, 6 filtros
    Saída da CONV1: 28x28x6
    POOL1 (MaxPool):
        f = 2, s = 2, 6 filtros
    Saída da POOL1 (MaxPool): 14x14x6
Camada 2:
    CONV2:
        f = 5, s = 1, 16 filtros
    Saída da CONV2: 10x10x16
    POOL2 (MaxPool):
        f = 2, s = 2, 16 filtros
    Saída da POOL2 (MaxPool): 5x5x16
Saída da segunda camada após unroll: 400x1
Camada 3: (FC3) -> camada padrão dos últimos cursos
    W3 é (120,400)
    b3 é (120,1)
Saída da camada 3: 120x1
Camada 4: (FC4) -> camada padrão dos últimos cursos
    W3 é (84,120)
    b3 é (84,1)
Saída da camada 4: 84x1
Camada 5: softmax
    W3 é (10,84)
    b3 é (10,1)
Saída da camada 5: 10x1   (0,1,...,9)

-> Conforme a profundidade da CNN vai aumentando os tamanhos n_h e n_w vão diminuindo (e n_c vai aumentando)
-> Padrão comum de CNN: CONV (1 ou mais) -> POOL -> CONV (1 ou mais) -> POOL -> FC -> FC -> FC -> Softmax

[Why Convolutions]
Exemplo:
    Entrada: 32x32x3 = 3072
        f = 5, 6 filtros
        Cada feature é 5x5 = 25
        Mais 1 parâmetro do bias
        (fl x fl x n_c{l-1} + 1) x n_cl = (5x5x3 + 1) x 6 = 456
    Saída: 28x28x6 = 4704
    (Se fosse camada FC, W teria dimensão 3072x4704 ~ 14M)

-> Compartilhamento de parâmetros: um detector de features (como um detector de linhas verticais) que é útil em uma parte da imagem provavelmente também é útil em outras partes
-> Esparsidade de conexões: Em cada camada, cada valor de saída depende apenas de um número pequeno de entradas



Deep Convolutional Models: Case Studies

[Why Look at Case Studies?]
-> Para ganhar intuição na criação de redes neurais convolucionais

Redes Clássicas:
    - LeNet-5
    - AlexNet
    - VGG

ResNet (152 camadas)

[Classic Networks]
-> LeNet-5: (Parecida com a arquitetura desenvolvida na semana anterior)        [LeCun et al., 1998. Gradient based learning applied to document recognition]
    - Reconhecimento de dígitos (0-9)
    - Imagem (32x32x1) -> [5x5, s=1] CONV (28x28x6) -> [f=2, s=2] POOL_AVG (14x14x6) -> [5x5, s=1] CONV (10x10x16) -> [f=2, s=2] POOL_AVG (5x5x16) -> FC (120) -> FC (84) -> yhat (10) [softmax]
    - 60k de parâmetros
    - Na época não era usado padding, então a dimensão ia diminuindo a cada camada e o número de canais ia aumentando
    OBS: Na época do paper eram usadas as funções de ativação sigmoid e tanh
    OBS2: Não linearidade após a camada POOL

-> AlexNet:     [Krizhevsky et al., 2012. ImageNet classification with deep convolutional neural networks]
    - Camadas: 
        -> Imagem (227x227x3) 
        -> [11x11, s=4] CONV (55x55x96) 
        -> [3x3, s=2] MAXPOOL (27x27x96) 
        -> [5x5, same] CONV (27x27x256) 
        -> [3x3, s=2] MAXPOOL (13x13x256)
        -> [3x3, same] CONV (13x13x384)
        -> [3x3] CONV (13x13x384)
        -> [3x3] CONV (13x13x256)
        -> [3x3, s=2] MAXPOOL (6x6x256)
        -> FC [unroll] (9216)
        -> FC (4096)
        -> FC (4096)
        -> Softmax (1000)
    - Parecida com a LeNet, mas muito maior
    - 60M de parâmetros
    - ReLU
    OBS: Multiple GPU
    OBS2: Local Response Normalization

-> VGG-16: (16 camadas com parâmetros)     [Simonyan & Zisserman, 2015. Very deep convolutional networks for large-scale image recognition]
    - Camadas de convolução no padrão: filtro 3x3, s = 1, same
    - Camadas de max-pooling no padrão: filtro 2x2, s = 2
    - Camadas:
        -> Imagem (224x224x3)
        -> CONV com 64 filtros (224x224x64)
        -> CONV com 64 filtros (224x224x64)
        -> MAXPOOL (112x112x64)
        -> CONV com 128 filtros (112x112x128)
        -> CONV com 128 filtros (112x112x128)
        -> MAXPOOL (56x56x128)
        -> CONV com 256 filtros (56x56x256)
        -> CONV com 256 filtros (56x56x256)
        -> CONV com 256 filtros (56x56x256)
        -> MAXPOOL (28x28x256)
        -> CONV com 512 filtros (28x28x512)
        -> CONV com 512 filtros (28x28x512)
        -> CONV com 512 filtros (28x28x512)
        -> MAXPOOL (14x14x512)
        -> CONV com 512 filtros (14x14x512)
        -> CONV com 512 filtros (14x14x512)
        -> CONV com 512 filtros (14x14x512)
        -> MAXPOOL (7x7x512)
        -> FC (4096)
        -> FC (4096)
        -> Softmax (1000)
    - 138M de parâmetros

[ResNets]
Redes neurais extremamente profundas são dificeis de treinar por conta do desaparecimento/explosão de gradientes
Usando "skip connections" podemos contruir uma ResNet (Residual Net), que possibilita o treinamento de redes neurais extremamente profundas (100+ camadas)
Residual block: 
    -> A ideia basicamente é colocar uma ativação al no cálculo da ativação de a{l+2} da forma:   (um "atalho"/"skip connection" no fluxo)
        a{l+2} = g(z{l+2} + al)

[Why ResNets Work]
Se incluirmos um "residual block" no final de uma grande rede neural a performance não será afetada, pois com a "skip connection" o bloco pode facilmente aprender a função identidade, retornando as mesmas ativações retornadas pela rede neural anterior ao bloco (essa propriedade é difícil de ser aprendida sem a utilização do bloco residual)

[Networks in Networks and 1x1 Convolutions]
Convolução com um filtro 1x1 -> multiplicação por escalar
Convolução com c filtros 1x1 -> multiplicação entre "vetores" de dimensão c (gerando um único valor a ser passado pela ReLU)   [operação não-trivial]
Pode ser usado para aumentar ou diminuir o número de canais

[Inception Network Motivation]
Inception: Usar diversos tamanhos de filtros e tanto CONV quanto MAXPOOL na mesma camada
    -> Custo computacional grande (exemplo com filtro 5x5)
        - 28x28x192 -> CONV com 32 filtros de 5x5 -> 28x28x32 (~120M de multiplicações)
    -> Para reduzir o número de multiplicações podemos usar a "bottleneck layer" para obter "o mesmo" resultado da convolução com filtro de 5x5
        - 28x28x192 -> CONV com 16 filtros de 1x1 -> 28x28x16 -> CONV com 32 filtros de 5x5 -> 28x28x32 (~12M de multiplicações)

[Inception Network]
Inception module:
    Ativação anterior (28x28x192) 
        '-> [64 filtros 1x1] CONV (28x28x64)
        '-> [96 filtros 1x1] CONV (28x28x96) -> [128 filtros 3x3] CONV (28x28x128)
        '-> [16 filtros 1x1] CONV (28x28x16) -> [32 filtros 5x5] CONV (28x28x32)
        '-> [3x3, s=1, same] MAXPOOL (28x28x192) -> [32 filtros 1x1] CONV (28x28x32)
    Concatenação de canais: (28x28x256) [Concatenar todos os anteriores]
Inception network é basicamente a combinação de diversos incepton modules

[Transfer Learning]
É possível encontrar no github diversas implementações de redes neurais com os pesos ótimos calculados
Dependendo da aplicação, podemos considerar uma transferência de aprendizado removendo a última camada (normalmente softmax) e "plugando" uma nova camada softmax no lugar contendo as classes de interesse

Para datasets pequenos:
    Os pesos dados pela rede baixada devem ser "congelados"
    Isso é, só devemos treinar a rede rasa dada pela saída desse pré-processamento com a nossa camada softmax

Para datasets médios:
    Congelamos menos camadas (as mais baixas) dadas pela rede neural baixada e treinamos as outras junto com a nossa camada softmax
    Ou descartamos as últimas camadas não congeladas e criamos as nossas próprias

Quando você tem muitos dados:
    Descartar a última camada (softmax) e incluir uma contendo a quantidade de classes do seu dataset
    Usar os pesos baixados como inicialização e treinar a rede neural inteira

[Data Augmentation]
Técnica normalmente usada em visão computacional para melhorar a performance -> Quanto mais dados melhor
Exemplos:
    -> Espelhar horizontalmente a imagem original
    -> Cortes aleatórios na imagem original (não é perfeito)
    -> Rotações na imagem original
    -> "Shearing"
    -> "Local Warping"
    -> "Color shifting"  (R <- +20; G <- -20; B <- +20)  [simulação de diferentes iluminações]
        - É possível usar PCA para escolher os vetores para alterar os valores RGB  [PCA color augmentation]

Também existem hiperparâmetros para essas alterações nas imagens originais

[State of Computer Vision]
Data vs. Hand-engineering:
    - Object Detection: Poucos dados disponíveis (mais desenvolvimento manual [hand-engineering, "hacks"] / transfer learning)
    - Image Recognition: Quantidade média de dados diponíveis
    - Speech Recognition: Grande quantidade de dados disponíveis (menos desenvolvimento manual / algoritmos mais simples)

Duas fontes de conhecimento:
    - Dados classificados [labeled]  (X, Y)
    - Hand engineered features / network architecture / other components

Dicas para benchmarks/competições:  [não é recomendado usar em produção]
    -> Ensembling: Treinar diversas redes (3 a 15) de forma independente e tirar a média das saídas (yhat)
    -> Multi-crop at test time: Rodar o classificador em múltiplas versões (cortes aleatórios) de imagens de teste e tirar a média dos resultados
        - Exemplo: 10-crop

Usar código aberto
    - Usar arquiteturas de redes publicadas em artigos/livros
    - Usar implementações de código aberto, se possível
    - Usar modelos pré-treinados e refiná-los para o seu dataset



Detection Algorithms

[Object Localization]
Detecção de um objeto:
    - Classificação: tem carro na imagem ou não?
    - Classificação com localização: tem carro na imagem ou não? Se tiver, localização com uma bounding box

Detecção de múltiplos objetos (uma ou mais categorias):
    - Detecção: muitos carros, localizados com bounding boxes

Exemplo de classificação com localização:
    Classes:
        1 - pedestre
        2 - carro
        3 - moto
        4 - background
    
    Saída:
        - Camada softmax com as probabilidades para as classes
        - bx, by, bh, bw (bounding box)

    y = [pc, bx, by, bh, bw, c1, c2, c3]
        pc - existe algum objeto (de alguma das classes (1,2,3))?
        bx - coordenada x do ponto central do objeto
        by - coordenada y do ponto central do objeto
        bh - altura do objeto
        bw - largura do objeto
        c1 - pedestre
        c2 - carro
        c3 - moto

    Loss simplificado (square error):
        L(yhat, y) = (yhat_1 - y_1)^2 + ... + (yhat_8 - y_8)^2   se y_1 = 1
        L(yhat, y) = (yhat_1 - y_1)^2                            se y_1 = 0

    O mais natural seria usarmos o Loss de regressão logística para pc, square error para (bx,by,bh,bw) e softmax para (c1,c2,c3), mas square error para todos também funciona

[Landmark Detection]
Definir pontos de interesse (landmarks) na imagem é uma forma de possibilitar algumas aplicações de classificação.
Exemplos:
    - Pontos que definem as linhas do rosto (delimitando olhos, boca, nariz e maxilar para capturar o formato do rosto)
        -> y = [pc, l1x, l1y, ..., l64x, l64y]
            pc = existe ou não um rosto na imagem?
            Onde (l1x, l1y) define o primeiro landmark (exemplo: o canto esquerdo do olho mais a esquerda da imagem)
            OBS: Os pontos devem ser consistentes entre todo o dataset (nesse caso l1 não poderia ser o canto direito do olho mais a esquerda, por exemplo)
        -> Esse mapeamento de landmarks permite a criação de aplicações de realidade aumentada, como filtros do snapchat
    - Pontos que definem a pose de uma pessoa (um ponto para cada ombro, joelho, pé, mão, ...)
        -> y = [pc, l1x, l1y, ..., l32x, l32y]

[Object Detection]
Detecção de objetos com "sliding windows"
Exemplo:
    - Treinar um classificador de carros (é um carro ou não?)
    - Definir um tamanho de janela para avaliarmos cada pedaço da imagem de entrada (com algum valor de stride definido)
    - Detectaremos um carro em cada janela onde a saída do classificador for 1
    -> Problemas:
        - Para um valor de stride muito pequeno podemos ter alto custo computacional (especialmente se o classificador for uma rede neural)
        - Um valor grande de stride diminui o custo computacional mas pode ser muito impreciso

OBS: Ao calcularmos o classificador em cada janela acabamos fazendo muitos cálculos repetidos

Existe uma solução para o custo computacional: O detector "sliding windows" pode ser implementado de forma mais eficiente com convolução

[Convolutional Implementation of Sliding Windows]
Podemos transformar camadas FC em camadas convolucionais:
    -> Com FC: 
        Entrada (14x14x3) -> [16 filtros 5x5] CONV (10x10x16) -> [16 filtros 2x2] MAXPOOL (5x5x16) -> FC (400) -> FC (400) -> Softmax (4)
    -> Transformando FC para CONV:
        Entrada (14x14x3) -> [16 filtros 5x5] CONV (10x10x16) -> [16 filtros 2x2] MAXPOOL (5x5x16) -> [400 filtros 5x5] CONV (1x1x400) -> [400 filtros 1x1] CONV (1x1x400) -> [4 filtros 1x1] CONV (1x1x4)
    OBS: Quando substituímos nossas camadas FC por camadas convolucionais ganhamos flexibilidade no tamanho da saída de todas as camadas

Implementação convolucional do "sliding windows":   [Sermanet et al., 2014, OverFeat: Integrated recognition, localization and detection using convolutional networks]
    -> Quando substituímos nossas camadas FC por camadas convolucionais podemos simplesmente usar a imagem completa como entrada (no lugar de uma janela)
    Exemplo: ("janela" de 14x14x3)
        - Entrada (14x14x3) -> CONV (10x10x16) -> MAXPOOL (5x5x16) -> "FC" (1x1x400) -> "FC" (1x1x400) -> "FC" (1x1x4)
        - Entrada (16x16x3) -> CONV (12x12x16) -> MAXPOOL (6x6x16) -> "FC" (2x2x400) -> "FC" (2x2x400) -> "FC" (2x2x4) 
                -> aqui temos as 2x2 saídas do classificador para cada janela 14x14 na imagem (stride = 2 por conta do MAXPOOL 2x2)
        - Entrada (28x28x3) -> CONV (24x24x16) -> MAXPOOL (12x12x16) -> "FC" (8x8x400) -> "FC" (8x8x400) -> "FC" (8x8x4)
                -> aqui temos as 8x8 saídas do classificador para cada janela 14x14 na imagem (stride = 2 por conta do MAXPOOL 2x2)
    OBS: A posição da bounding box ainda não é tão precisa

[Bounding Box Predictions]
Algoritmo YOLO (You Only Look Once):    [Redmon et al., 2015, You Only Look Once: Unified real-time object detection]
    -> Identifica de forma mais precisa a posição da bounding box
    -> Intuição: 
        - Podemos pensar na imagem dividida em várias regiões (um grid 3x3 por exemplo)
        - E em cada região avaliamos o classificador definido na primeira aula dessa semana (saída y = [pc, bx, by, bh, bw, c1, c2, c3])
    -> Implementação é feita de forma convolucional (ou seja, não avaliamos o mesmo classificador uma vez em cada célula):
        - Exemplo: (grid 3x3)
            -> A saída que queremos tem formato 3x3x8 (isso é, 8 parâmetros [pc, bx, by, bh, bw, c1, c2, c3] para cada célula do grid)
            -> Entrada (100x100x3) -> CONV -> MAXPOOL -> ... -> Saída (3x3x8)
        - O objeto detectado deve ser atribuído à célula do grid onde se encontra seu ponto central (bx,by)
        - A convenção é que em cada célula temos pontos cujas coordenadas estão no intervalo (0, 1)
        OBS: bh e bw podem ser maiores que 1, mas bx e by não

[Intersection Over Union]
Uma forma de medir o desempenho do algoritmo de detecção é calcular o IoU:
    IoU = (Interseção entre a bounding box prevista e a classificada) / (União entre a bounding box prevista e a classificada)
    Threshold padrão: Previsão correta se IoU >= 0.5

[Non-max Suppression]
Quando definimos um grid de alta granularidade para o algoritmo YOLO podemos ter diversas células vizinhas com pc > 0.6 (threshold escolhido)
Uma forma de descartar as células que se referem ao mesmo objeto é calcular IoU entre a bounding box que possui o pc máximo (escolhida como a delimitadora da posição do objeto) e as bounding boxes restantes e descartá-las quando IoU >= 0.5

[Anchor Boxes]
O problema do algoritmo que estamos especificando até agora é que ele é capaz de detectar apenas uma classe de objeto por célula
Podemos escolher mais de um formato de bounding box para considerarmos no aprendizado e identificarmos mais de um objeto por célula
Exemplo com dois formatos:
    Saída: y = [pc1, bx1, by1, bh1, bw1, c11, c21, c31, pc2, bx2, by2, bh2, bw2, c12, c22, c32]    (3x3x16 ou 3x3x2x8  [2 é o número de anchors])
        (Onde pc1, bx1, ..., c31 se refere ao primeiro bounding box (um retângulo deitado, por exemplo) e pc2, bx2, ..., c32 se refere ao segundo bounding box (um retângulo em pé))
    Atribuímos cada objeto à célula onde seu ponto central se encontra e o número da bounding box com maior IoU com o objeto
        Ou seja, agora temos um par (célula, boundingBox) e podemos identificar até dois objetos em cada célula, desde que seus formatos sejam consideravelmente distintos (neste caso, um objeto deve ser mais alto do que largo e o outro mais largo do que alto)

[YOLO Algorithm]
Exemplo: (grid de 3x3 e 2 bounding boxes [retângulo vertical (1) e horizontal (2)])
    -> Classes:
        - Pedestre (c1)
        - Carro (c2)
        - Moto (c3)
    -> y é 3x3x16 ([pc1, bx1, by1, bh1, bw1, c11, c21, c31, pc2, bx2, by2, bh2, bw2, c12, c22, c32])
    -> Treinamento
        - Célula sem objetos terá y = [0, ?, ?, ?, ?, ?, ?, ?, 0, ?, ?, ?, ?, ?, ?, ?]  (? é "don't care")
        - Célula contendo a traseira de um carro (bounding box horizontal se encaixa melhor) terá y = [0, ?, ?, ?, ?, ?, ?, ?, 1, bx, by, bh, bw, 0, 1, 0]
    -> Esquema simplificado da rede: Entrada (100x100x3) -> ConvNet -> Saída (3x3x16)
    -> Saída "non-max suppressed":
        - Para cada célula do grid obter os 2 bounding boxes previstos
        - Descartar as probabilidades baixas (pc <= 0.6 por exemplo)
        - Para cada classe use "non-max suppression" (de forma independente) para gerar as predições finais
            -> Isso é, descartar as bounding boxes onde o IoU entre a bounding box de pc máximo e as outras bounding boxes é maior que 0.5

[Region Proposals]
Region Proposal (R-CNN):    [Girshik et al., 2013, Rich feature hirarchies for accurate object detection and semantic segmentation]
    Tenta pegar apenas algumas janelas de interesse (regiões propostas) para executar o "sliding windows"
    As regiões (blobs) são dadas após a execução de um algoritmo de segmentação

OBS: Esse algoritmo é consideravelmente lento

Algoritmos mais rápidos:
    Fast R-CNN:     [Girshik, 2015. Fast R-CNN]
        - Basicamente o R-CNN utilizando a implementação convolucional de "sliding windows"
        - Continua lento por depender da execução prévia do algoritmo de segmentação
    Faster R-CNN:   [Ren et al., 2016, Faster R-CNN: Towards real-time object detection with region proposal networks]
        - Usa redes convolucionais para propor as regiões
        OBS: As implementações ainda são mais lentas que as do algoritmo YOLO



Special Applications: Face Recognition & Neural Style Transfer


-> Face Recognition

[What is Face Recognition?]
Face verification:
    - Entrada: Imagem e nome (ou ID)
    - Saída: Informa se a imagem é da pessoa cujo nome foi dado

Face Recognition:
    - Tem uma base de K pessoas
    - Entrada: Imagem
    - Saída: ID da pessoa se ela é uma das K pessoas e "Não reconhecido" caso contrário

[One Shot Learning]
É necessário que a aplicação seja capaz de reconhecer a pessoa tendo aprendido apenas com uma imagem de cada pessoa
Não é uma boa ideia treinarmos uma rede convolucional tendo como entrada a imagem da pessoa e na saída uma camada softmax com a quantidade de classes igual a quantidade de pessoas na base pois o dataset é extremamente pequeno
É melhor treinarmos uma rede para aprender uma função de "similaridade": d(img1, img2) = grau de similaridade entre as imagens
Definimos um threshold t para o qual: (face verification)
    - Se d(img1, img2) <= t então consideramos que as duas pessoas são iguais 
    - Se d(img1, img2) > t então consideramos que as duas pessoas são diferentes

[Siamese Network]
É uma arquitetura de rede onde calculamos as saídas para duas imagens (x1 e x2) e depois comparamos suas saídas (ou encodings) f(x) 
Os parâmetros da NN definem um encoding f(x) de tamanho 128: [Taigman et al., 2014. DeepFace closing the gap to human level performance]
    d(x1, x2) = ||f(x1) - f(x2)||^2
Queremos que:
    - Se xi e xj são a mesma pessoa, então ||f(xi) - f(xj)||^2 é pequeno
    - Se xi e xj são pessoas diferentes, então ||f(xi) - f(xj)||^2 é grande

[Triplet Loss]          [Schroff et al., 2015, FaceNet: A unified embedding for face recognition and clustering]
A ideia básica é que sempre olharemos para três imagens por vez:
    - Uma fixa [A], que será comparada com as outras duas
    - Um exemplo positivo [P] (mesma pessoa que da imagem fixa)
    - Um exemplo negativo [N] (uma pessoa diferente da imagem fixa)

Objetivo: ||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + alpha <= 0       (alpha é chamado de margem, um hiperparâmetro)
Loss: L(A, P, N) = max(||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + alpha, 0)
    J = sum(L(Ai, Pi, Ni))

É necessário garantir que as tríades de imagens escolhidas sejam "dificeis" para que a rede seja bem treinada
Isso é, d(A, P) ~ d(A, N)

[Face Verification and Binary Classification]       [Taigman et al., 2014. DeepFace closing the gap to human level performance]
Existe outra forma de treinar os parâmetros que consiste na conexão das últimas camadas das duas redes (uma para cada imagem) através de uma unidade logística:
    yhat = sigmoid(sum(w_k * |f(xi)_k - f(xj)_k| + b))
    (também podemos substituir |f(xi)_k - f(xj)_k| por (f(xi)_k - f(xj)_k)^2 / (f(xi)_k + f(xj)_k)   [similaridade qui-quadrado])
    OBS: Aqui a entrada é composta por duas imagens
    OBS2: Para predição podemos pré-computar as saídas f(xj) dos funcionários presentes na base para melhorar o desempenho 
            (não existe necessidade de armazenar as imagens nessa abordagem [pode ser usada com triplet loss também])


-> Neural Style Transfer

[What is Neural Style Transfer]
Na transferência de estilo de uma imagem para outra temos 3 imagens:
    -> C: Imagem para a qual queremos transferir o estilo
    -> S: Imagem com o estilo a ser transferido
    -> G: Imagem gerada (C com o estilo de S)

[What Are Deep ConvNets Learning?]
Para visualizar o que uma ConvNet está aprendendo realizamos o procedimento:
    - Escolher uma unidade na primeira camada 
    - Encontrar os 9 fragmentos de imagem que maximizam a unidade escolhida
    -> Repetir para outras unidades

(A cada camada as detecções ficam mais complexas)

[Cost Function]     [Gatys et al., 2015, A neural algorithm of artistic style]
Mede quão boa é a imagem G gerada: 
    J(G) = alpha * Jcontent(C,G) + beta * Jstyle(S,G)

Algoritmo simplificado:
    - Inicializar aleatoriamente a matriz G (100x100x3)
    - Usar gradiente decrescente para minimizar J(G)

[Content Cost Function]
Queremos usar a camada l de uma rede convolucional pretreinada (VGG, por exemplo) para computar o custo do conteúdo
Definimos al_c e al_g como a ativação da camada l nas imagens C e G  (al_g and al_c UNROLLED INTO VECTORS)
Se al_c e al_g são similares então as duas imagens possuem conteúdo similar:
    JContent(C,G) = ||al_c - al_g||^2 / 2

[Style Cost Function]
Estilo é definido como a correlação entre as ativações através dos canais

Style Matrix (Gram Matrix):
    Gl(k,k') = sum(sum(al_g(i,j,k) * al_g(i,j,k')))     [Gl é nl_c x nl_c]

JStyle(S,G) = ||Gl_S - Gl-G||^2 / (2 * nl_h * nl_w * nl_c) = sum(sum((Gl_S(k,k') - Gl_G(k,k'))^2)) / (2 * nl_h * nl_w * nl_c)
(||Gl_S - Gl-G||^2 é a norma frobenius)

Uma média entre as camadas dá resultados melhores:
    JStyleTotal(S,G) = sum(lambda_l * JStyle(S,G)  para l)

[1D and 3D Generalizations]
Convolução também pode ser aplicada em dados de outras dimensões
Exemplo: Áudio (1D) [para a maioria das aplicações com dados 1D são usadas redes neurais recorrentes, mas ConvNets também podem ser usadas]
Exemplo: Vídeo (3D) [altura, largura e tempo]